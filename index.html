---
layout: default
---

<div class="home">
  <h1>Lexicase Selection</h1>

  <p>Lexicase selection is a selection method first developed as a parent selection method for evolutionary algorithms, specifically genetic programming. Unlike traditional selection methods that aggregate multiple selection criteria (often errors) across all training cases, lexicase selection separately considers performance of individual training cases.</p>

  <p><strong>Python Package Available:</strong> <a href="https://pypi.org/project/lexicase/">Install from PyPI</a></p>

  <h2>How It Works</h2>
  <p><em>The following describes vanilla lexicase selection. See <a href="#variants">variants</a> for modified versions.</em></p>
  <ol>
    <li><strong>Random Ordering</strong>: For each selection event, the training cases are randomly shuffled</li>
    <li><strong>Sequential Filtering</strong>: Candidates are filtered based on performance on each training case in order</li>
    <li><strong>Best Performance</strong>: Only individuals with the best performance on the current training case advance</li>
    <li><strong>Final Selection</strong>: The process continues until one individual remains or all training cases are considered</li>
  </ol>

  <h2>Advantages</h2>
  <ul>
    <li><strong>Avoids averaging traps</strong>: Non-aggregating selection prevents getting stuck in local optima caused by balancing mediocre performance across training cases, enabling richer exploration of the solution space</li>
    <li><strong>Preserves diversity and specialists</strong>: Random ordering of training cases combined with non-aggregation allows specialized solutions to survive and reproduce based on excellence in specific areas, maintaining population diversity</li>
    <li><strong>Handles multidimensional problems naturally</strong>: Eliminates the need to weight competing objectives or collapse multiple criteria into a single fitness score</li>
  </ul>

  <h2>Applications</h2>
  <p>Lexicase selection has been successfully applied to:</p>
  <ul>
    <li>Program synthesis</li>
    <li>Symbolic regression (using epsilon lexicase selection)</li>
    <li>Deep learning (using gradient lexicase selection)</li>
    <li>Multi-objective optimization</li>
  </ul>

  <h2>Algorithm Pseudocode</h2>
  <pre><code>function lexicase_selection(population, training_cases):
    shuffle(training_cases)  // Random ordering of training cases
    candidates = copy(population)
    
    for each training_case in training_cases:
        if length(candidates) <= 1:
            break
            
        best_performance = find_best_performance(candidates, training_case)
        candidates = filter_by_performance(candidates, training_case, best_performance)
    
    return random_choice(candidates)</code></pre>

  <h2>Implementation Notes</h2>
  
  <h3>Performance Evaluation</h3>
  <ul>
    <li>Each individual must be evaluated on all training cases</li>
    <li>Performance can be measured as fitness, error, or any other metric</li>
    <li>Lower values typically indicate better performance (minimization)</li>
  </ul>

  <!---
  <h3>Training Case Management</h3>
  <ul>
    <li>Training cases should ideally be diverse and representative</li>
    <li>The number of training cases affects selection pressure</li>
    <li>Dynamic training case generation can be beneficial</li>
  </ul>
  --->

  <h2 id="variants">Variants</h2>
  
  <h3>Epsilon Lexicase</h3>
  <p>Designed specifically for continuous-valued errors (while standard lexicase is best suited for problems in discrete domains, for example with Boolean or integer errors), epsilon lexicase selection allows individuals within an epsilon threshold to be considered equivalent:</p>
  <pre><code>best_performance = find_best_performance(candidates, training_case)
threshold = best_performance + epsilon
candidates = filter_by_threshold(candidates, training_case, threshold)</code></pre>

  <p><em>Note: In the recommended form of epsilon lexicase selection, we keep not only the candidates with exactly the best performance of any individual currently in the set (on the given case), but also any other candidates with performance that differs from the best performance (on that case) by less than epsilon, where epsilon is the median absolute difference between errors on the case and the median error for the case, computed over the entire population, once per generation.</em></p>

  <h3>Down-sampled Lexicase</h3>
  <p>Uses a subset of training cases for each selection event to reduce computational cost.</p>

  <h3>DALex (Diversely Aggregated Lexicase)</h3>
  <p>In DALex, training cases <i><b></b>are</b></i> aggregated, but only after randomized weighting. When weight differences are large, DALex's behavior approaches that of ordinary lexicase selection. This permits significant efficiency improvements and provides a "particularity pressure" hyperparameter that can be adjusted for better performance in some environments.</p> 
  <!--- <p>DALex combines aspects of lexicase selection with diverse aggregation methods. Instead of using a single random ordering of training cases, DALex uses multiple diverse aggregation functions to evaluate candidates, maintaining the benefits of lexicase while incorporating aggregation diversity.</p> --->

  <h3>Plexicase (Probabilistic Lexicase)</h3>
  <p>Plexicase introduces probabilistic elements to lexicase selection, allowing for more flexible selection behavior while maintaining the core principles of case-by-case evaluation. This variant can help balance exploration and exploitation in the selection process.</p>

  <h2>Parameters</h2>
  <table>
    <thead>
      <tr>
        <th>Parameter</th>
        <th>Description</th>
        <th>Typical Values</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Population Size</td>
        <td>Number of individuals</td>
        <td>100-1000</td>
      </tr>
      <tr>
        <td>Training Cases</td>
        <td>Number of evaluation cases</td>
        <td>10-100+</td>
      </tr>
      <tr>
        <td>Epsilon</td>
        <td>Tolerance threshold</td>
        <td>0.0-1.0</td>
      </tr>
      <tr>
        <td>Sample Rate</td>
        <td>Fraction of training cases used</td>
        <td>0.1-1.0</td>
      </tr>
    </tbody>
  </table>

  <h2>Performance Considerations</h2>
  <ul>
    <li><strong>Time Complexity</strong>: O(n × m) where n is population size and m is number of training cases, <a href="https://arxiv.org/abs/2204.06461">although empirically it is generally much better</a></li>
    <li><strong>Memory</strong>: Requires storing performance on all training cases</li>
    <li><strong>Parallelization</strong>: Training case evaluation can be parallelized</li>
  </ul>

 <!---
  
  <h2>Example Use Cases</h2>
  
  <h3>Program Synthesis</h3>
  <pre><code># Training cases for sorting function
training_cases = [
    ([3, 1, 4], [1, 3, 4]),
    ([5, 2], [2, 5]),
    ([], []),
    ([1], [1])
]</code></pre>

  <h3>Symbolic Regression</h3>
  <pre><code># Training cases for mathematical function
training_cases = [
    (x=0, expected=1),
    (x=1, expected=2),
    (x=2, expected=5),
    (x=3, expected=10)
]</code></pre>
--->

  <!---
  <h2>Research</h2>
  <p>This selection method has been the subject of extensive research in the evolutionary computation community. It has shown particular promise in domains where maintaining diversity is crucial for finding high-quality solutions.</p>
  --->
  
  <h2>References</h2>
  <ul>
    <li>Spector, L. (2012). Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming: A Preliminary Report. In Companion Publication of the 2012 Genetic and Evolutionary Computation Conference, GECCO’12 Companion. ACM Press. pp. 401 - 408.</li>
    <li>Helmuth, T., L. Spector, and J. Matheson. (2014). Solving Uncompromising Problems with Lexicase Selection. In IEEE Transactions on Evolutionary Computation, vol. 19, no. 5, pp. 630 - 643.</li>
    <li>La Cava, W., L. Spector, and K. Danai (2016). Epsilon-lexicase selection for regression. GECCO '16: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 741 - 748.</li>
    <li>Hernandez, J. G., A. Lalejini, E. Dolson, and C. Ofria (2019). Random subsampling improves performance in lexicase selection. GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 2028 - 2031.</li>
    <li>Ding, L., & Spector, L. (2022). Optimizing Neural Networks with Gradient Lexicase Selection. In The Tenth International Conference on Learning Representations, ICLR 2022.</li>
    <li>Ni, A., Ding, L., & Spector, L. (2024, March). Dalex: Lexicase-like selection via diverse aggregation. In European Conference on Genetic Programming (Part of EvoStar) (pp. 90-107).</li>
    <li>Ding, L., Pantridge, E., & Spector, L. (2023, July). Probabilistic lexicase selection. In Proceedings of the genetic and evolutionary computation conference (pp. 1073-1081).</li>
    <li><em>See also: <a href="https://scholar.google.com/scholar?hl=en&q=lexicase+selection">Lexicase Selection on Google Scholar</a>.</em></li>
  </ul>

  <h2>Resources</h2>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=Th6Hx3SJOlk">Lexicase selection tutorial by Tom Helmuth and Bill La Cava from GECCO '21</a></li>
    <li><a href="https://pypi.org/project/lexicase/">Python Package on PyPI</a></li>
    <li><a href="https://github.com/nlorant-s/lexicase.github.io">GitHub Repository for Lexicase Documentation</a></li>
  </ul>

  <hr>
  <p><em>Last updated: October 2025</em></p>
</div>















