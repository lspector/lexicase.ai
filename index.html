---
layout: default
---

<div class="home">
  <h1>Lexicase Selection</h1>

  <p>Lexicase selection is a selection method first developed as a parent selection method for evolutionary algorithms, specifically genetic programming. It has since been applied to several other forms of evolutionary computing and to other machine learning algorithms. Unlike traditional selection methods that aggregate multiple selection criteria (often errors) across all training cases, lexicase selection separately considers performance of individual training cases.</p>

  <p><strong>Python Package Available:</strong> <a href="https://pypi.org/project/lexicase/">Install from PyPI</a></p>

  <h2>How It Works</h2>
  <p><em>The following describes ordinary lexicase selection method. See <a href="#variants">variants</a> for modified versions.</em></p>
  <ol>
    <li><strong>Random Ordering</strong>: For each selection event, the training cases are randomly shuffled</li>
    <li><strong>Sequential Filtering</strong>: Candidates are filtered based on performance on each training case in order</li>
    <li><strong>Best Performance</strong>: Only individuals with the best performance on the current training case advance</li>
    <li><strong>Final Selection</strong>: The process continues until one individual remains or all training cases are considered</li>
  </ol>

  <h2>Advantages</h2>
  <ul>
    <li><strong>Avoids averaging traps</strong>: Non-aggregating selection prevents getting stuck in local optima caused by balancing mediocre performance across training cases, enabling richer exploration of the solution space</li>
    <li><strong>Preserves diversity and specialists</strong>: Random ordering of training cases combined with non-aggregation allows specialized solutions to survive and reproduce based on excellence in specific areas, maintaining population diversity</li>
    <li><strong>Handles multidimensional problems naturally</strong>: Eliminates the need to weight competing objectives or collapse multiple criteria into a single fitness score</li>
  </ul>

  <h2>Applications</h2>
  <p>Lexicase selection has been successfully applied to:</p>
  <ul>
    <li>Program synthesis</li>
    <li>Symbolic regression (using epsilon lexicase selection)</li>
    <li>Deep learning (using gradient lexicase selection)</li>
    <li>Multi-objective optimization</li>
    <li>Reinforcement learning</li>
    <li>Learning classifier systems</li>
  </ul>

  <h2>Algorithm Pseudocode</h2>
  <pre><code>function lexicase_selection(population, training_cases):
    shuffle(training_cases)  // Random ordering of training cases
    candidates = copy(population)
    
    for each training_case in training_cases:
        if length(candidates) <= 1:
            break
            
        best_performance = find_best_performance(candidates, training_case)
        candidates = filter_by_performance(candidates, training_case, best_performance)
    
    return random_choice(candidates)</code></pre>

  <h2 id="variants">Variants</h2>
  
  <h3>Epsilon Lexicase</h3>
  <p>Designed specifically for continuous-valued errors (while standard lexicase is best suited for problems in discrete domains, for example with Boolean or integer errors), epsilon lexicase selection allows individuals within an epsilon threshold to be considered equivalent:</p>
  <pre><code>best_performance = find_best_performance(candidates, training_case)
threshold = best_performance + epsilon
candidates = filter_by_threshold(candidates, training_case, threshold)</code></pre>

  <p><em>Note: In the recommended form of epsilon lexicase selection, we keep not only the candidates with exactly the best performance of any individual currently in the set (on the given case), but also any other candidates with performance that differs from the best performance (on that case) by less than epsilon, where epsilon is the median absolute difference between errors on the case and the median error for the case, computed over the entire population, once per generation.</em></p>

  <h3>Down-sampled Lexicase</h3>
  <p>Uses a subset of training cases for each selection event to reduce computational cost.</p>

  <h3>DALex (Diversely Aggregated Lexicase)</h3>
  <p>In DALex, training cases <i><b></b>are</b></i> aggregated, but only after randomized weighting. When weight differences are large, DALex's behavior approaches that of ordinary lexicase selection. This permits significant efficiency improvements and provides a "particularity pressure" hyperparameter that can be adjusted for better performance in some environments.</p> 
  <!--- <p>DALex combines aspects of lexicase selection with diverse aggregation methods. Instead of using a single random ordering of training cases, DALex uses multiple diverse aggregation functions to evaluate candidates, maintaining the benefits of lexicase while incorporating aggregation diversity.</p> --->

  <h3>Plexicase (Probabilistic Lexicase)</h3>
  <p>Approximates the selection probabilities for each individual under lexicase selection, and then samples from that distributuion instead of performing lexicase selection's repeated filtering steps.
    Behaves similarly to lexicase selection but is significantly more efficient. Also allows for adjustment of the kurtosis of the distribution, which can be beneficial in some problem environments.</p>
  
  <!--- <p>Plexicase introduces probabilistic elements to lexicase selection, allowing for more flexible selection behavior while maintaining the core principles of case-by-case evaluation. This variant can help balance exploration and exploitation in the selection process.</p> --->

  <h3>Gradient Lexicase</h3>
  <p>Integrates lexicase selection into deep learning by simultaneously performing gradient descent on several copies of the model, each trained with respect to a different subset of the training data, and then performing lexicase selection to determine which model to carry forward to the next learning epoch.</p>

  <h3> Batch Lexicase</h3>
  Aggregates errors within randomly-chosen batches of training cases, and then treats the batches as individual training cases for lexicase selection. May be useful for noisy training data.
  
  <h2>Time Complexity</h2>
  The time complexity of ordinary lexicase selection is <i>O(n × m)</i> where <i>n</i> is population size and <i>m</i> is number of training cases.
  However, the empirically observed runtimes are generally significantly better than this basic complexity analysis
  would lead one to expect. Explanations for the better-than-expected performance are explored in <a href="https://arxiv.org/abs/2204.06461">this paper</a>. DALex and Plexicase provide other high performance options.</p>
  
  <h2>References</h2>
  <ul>
    <li>Spector, L. (2012). Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming: A Preliminary Report. In <i>Companion Publication of the 2012 Genetic and Evolutionary Computation Conference, GECCO’12 Companion</i>, pp. 401-408.</li>
    <li>Helmuth, T., L. Spector, and J. Matheson. (2014). Solving Uncompromising Problems with Lexicase Selection. In <i>IEEE Transactions on Evolutionary Computation</i>, vol. 19, no. 5, pp. 630-643.</li>
    <li>La Cava, W., L. Spector, and K. Danai (2016). Epsilon-lexicase selection for regression. <i>GECCO '16: Proceedings of the Genetic and Evolutionary Computation Conference</i>, pp. 741-748.</li>
    <li>Hernandez, J. G., A. Lalejini, E. Dolson, and C. Ofria (2019). Random subsampling improves performance in lexicase selection. <i>GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference Companion</i>, pp. 2028-2031.</li>
    <li>Aenugu, S., & Spector, L. (2019). Lexicase selection in learning classifier systems. In <i>GECCO '19: Proceedings of the Genetic and Evolutionary Computation Conference</i>, pp. 356–364.</li>
    <li>Ding, L., & Spector, L. (2022). Optimizing Neural Networks with Gradient Lexicase Selection. In The Tenth International Conference on Learning Representations, ICLR 2022.</li>
    <li>Helmuth, T., Lengler, J., & La Cava, W. (2022). Population Diversity Leads to Short Running Times of Lexicase Selection. In <i>Parallel Problem Solving from Nature – PPSN XVII: 17th International Conference, PPSN 2022</i>, pp. 485-498.</li>
    <li>Ding, L., Pantridge, E., & Spector, L. (2023). Probabilistic lexicase selection. In <i>GECCO '23: Proceedings of the Genetic and Evolutionary Computation Conference</i>, pp. 1073-1081.</li>
    <li>Ni, A., Ding, L., & Spector, L. (2024). Dalex: Lexicase-like selection via diverse aggregation. In <i>European Conference on Genetic Programming</i> (EuroGP, part of EvoStar), pp. 90-107.</li>
    <li>Bahlous-Boldi, R., Ding, L., Spector, L., & Niekum, S. (2025). Pareto Optimal Learning from Preferences with Hidden Context. In <i>Reinforcement Learning Conference, RLC 2025</i>.</li>
    <li><em>See also: <a href="https://scholar.google.com/scholar?hl=en&q=lexicase+selection">Lexicase Selection on Google Scholar</a>.</em></li>
  </ul>

  <h2>Resources</h2>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=Th6Hx3SJOlk">Lexicase selection tutorial by Tom Helmuth and Bill La Cava from GECCO '21</a></li>
    <li><a href="https://pypi.org/project/lexicase/">Python Package on PyPI</a></li>
    <!--- <li><a href="https://github.com/nlorant-s/lexicase.github.io">GitHub Repository for Lexicase Documentation</a></li> --->
  </ul>

  <hr>
  <p><em>Last updated: October 2025</em></p>
</div>



























